{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6093ed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.11.0-cp311-cp311-win_amd64.whl (884 kB)\n",
      "     -------------------------------------- 884.4/884.4 kB 2.8 MB/s eta 0:00:00\n",
      "Collecting regex>=2022.1.18\n",
      "  Downloading regex-2025.7.34-cp311-cp311-win_amd64.whl (276 kB)\n",
      "     -------------------------------------- 276.0/276.0 kB 4.2 MB/s eta 0:00:00\n",
      "Collecting requests>=2.26.0\n",
      "  Downloading requests-2.32.4-py3-none-any.whl (64 kB)\n",
      "     ---------------------------------------- 64.8/64.8 kB 1.7 MB/s eta 0:00:00\n",
      "Collecting charset_normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.4.3-cp311-cp311-win_amd64.whl (107 kB)\n",
      "     -------------------------------------- 107.1/107.1 kB 3.1 MB/s eta 0:00:00\n",
      "Collecting idna<4,>=2.5\n",
      "  Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
      "     ---------------------------------------- 70.4/70.4 kB 4.0 MB/s eta 0:00:00\n",
      "Collecting urllib3<3,>=1.21.1\n",
      "  Downloading urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "     -------------------------------------- 129.8/129.8 kB 8.0 MB/s eta 0:00:00\n",
      "Collecting certifi>=2017.4.17\n",
      "  Downloading certifi-2025.8.3-py3-none-any.whl (161 kB)\n",
      "     -------------------------------------- 161.2/161.2 kB 3.2 MB/s eta 0:00:00\n",
      "Installing collected packages: urllib3, regex, idna, charset_normalizer, certifi, requests, tiktoken\n",
      "Successfully installed certifi-2025.8.3 charset_normalizer-3.4.3 idna-3.10 regex-2025.7.34 requests-2.32.4 tiktoken-0.11.0 urllib3-2.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install tiktoken "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90d2b0ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\suryansh khare\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (22.3)\n",
      "Collecting pip\n",
      "  Downloading pip-25.2-py3-none-any.whl (1.8 MB)\n",
      "     ---------------------------------------- 1.8/1.8 MB 1.2 MB/s eta 0:00:00\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 22.3\n",
      "    Uninstalling pip-22.3:\n",
      "      Successfully uninstalled pip-22.3\n",
      "Successfully installed pip-25.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94433007",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV2:\n",
    "    def __init__(self,vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = { i:s for s,i in vocab.items()}\n",
    "    def encode(self,text):\n",
    "        preprocessed = preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)',text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.split()]\n",
    "        preprocessed = [\n",
    "            item if item in self.str_to_int\n",
    "            else \"|unk|\" for item in preprocessed\n",
    "        ]\n",
    "\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "    def decode(self,ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        #Replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])',r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "519a5bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.11.0\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import tiktoken\n",
    "\n",
    "print(\"tiktoken version:\", importlib.metadata.version(\"tiktoken\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "904fa83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d14699",
   "metadata": {},
   "source": [
    "# Here some unknown plce is not there in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ba449d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 466, 345, 588, 8887, 30, 1279, 91, 437, 286, 2420, 91, 29, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271]\n"
     ]
    }
   ],
   "source": [
    "text = (\n",
    "    \"Hello, do you like tea? <|end of text|> In the sunlit terraces\"\n",
    "    \"of someunknownPlace\"\n",
    ")\n",
    "\n",
    "integers = tokenizer.encode(text,allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ea05d6",
   "metadata": {},
   "source": [
    "# Exactly similar text is made to output through the decode method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa834f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|end of text|> In the sunlit terracesof someunknownPlace\n"
     ]
    }
   ],
   "source": [
    "strings = tokenizer.decode(integers)\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade91606",
   "metadata": {},
   "source": [
    "# Another example for showing the workin of the BPE Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5d427a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33901, 86, 14246, 220, 959]\n",
      "Akwiw ier\n"
     ]
    }
   ],
   "source": [
    "integers = tokenizer.encode(\"Akwiw ier\")\n",
    "print(integers)\n",
    "\n",
    "a = tokenizer.decode(integers)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c45b376",
   "metadata": {},
   "source": [
    "#We can take a noteworthu observation based on the token ids and decoded text above that the First  the <|end of text|> token is assigned a relatively large token ID namely 50256 The BPE tokenizers which was used to train models such as GPT 2 and GPT 3  has a total vocab size of 50256."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcae176",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
