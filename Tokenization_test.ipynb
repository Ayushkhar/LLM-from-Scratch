{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32dfde54",
   "metadata": {},
   "source": [
    "<!-- Reads first 100 characters -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d722f71d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total numbers of character: 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "with open(r\"D:\\Machine Learning\\LLM from Scratch\\the-verdict.txt\",\"r\",encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "print(\"Total numbers of character:\",len(raw_text))\n",
    "print(raw_text[:99])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0633d807",
   "metadata": {},
   "source": [
    "<!-- #Our Goal is to tokenize these 20479 character short story into indivisual words and special characters that we can then turn them into embeddings for LLM trining -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70c4fc16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ' ', 'World.', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"Hello World. This, is a test.\"\n",
    "result = re.split(r'(\\s)',text) # (\\s) to skip the witespaces\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d193386e",
   "metadata": {},
   "source": [
    "<!-- We will also modify the reg expression splits on whitespaces(\\s) and commas and periods  -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e1869f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ' ', 'World', '.', '', ' ', 'This', ',', '', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']\n"
     ]
    }
   ],
   "source": [
    "result = re.split(r'([,.]|\\s)',text)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d6651f",
   "metadata": {},
   "source": [
    "<!-- A small remaining issue is that the list still includes whitespace characters. Optionally, we\n",
    "can remove these redundant characters safely as follows: -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b1a84ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'World', '.', 'This', ',', 'is', 'a', 'test', '.']\n"
     ]
    }
   ],
   "source": [
    "result = [item for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca11983",
   "metadata": {},
   "source": [
    "<!-- The tokenization scheme we devised above works well on the simple sample text. Let's\n",
    "modify it a bit further so that it can also handle other types of punctuation, such as\n",
    "question marks, quotation marks, and the double-dashes we have seen earlier in the first\n",
    "100 characters of Edith Wharton's short story, along with additional special characters: -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3273d53e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Hello'], [','], ['world'], ['.'], ['Is'], ['this'], ['--'], ['a'], ['test'], ['?']]\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, world. Is this-- a test?\"\n",
    "result = re.split(r'(--|[,.:?_!\"()\\[\\]\\s])', text)\n",
    "result = [item.split() for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a4e087",
   "metadata": {},
   "source": [
    "<!-- Now we will apply the above tokenizer we built to preprocess the book we had in raw_text -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "383ff589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
     ]
    }
   ],
   "source": [
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)',raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.split()]\n",
    "print(preprocessed[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3196be70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4690\n"
     ]
    }
   ],
   "source": [
    "print(len(preprocessed)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa26add",
   "metadata": {},
   "source": [
    "<!--Step 2 -> Create the Token ID's -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d10585e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1130\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9192c7cb",
   "metadata": {},
   "source": [
    "<!-- After determining the vocabulary size is 1130 we create vocabulary and assign Token ID -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a32c52eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {token:integer for integer,token in enumerate(all_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9be0fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n",
      "('.', 7)\n",
      "(':', 8)\n",
      "(';', 9)\n",
      "('?', 10)\n",
      "('A', 11)\n",
      "('Ah', 12)\n",
      "('Among', 13)\n",
      "('And', 14)\n",
      "('Are', 15)\n",
      "('Arrt', 16)\n",
      "('As', 17)\n",
      "('At', 18)\n",
      "('Be', 19)\n",
      "('Begin', 20)\n",
      "('Burlington', 21)\n",
      "('But', 22)\n",
      "('By', 23)\n",
      "('Carlo', 24)\n",
      "('Chicago', 25)\n",
      "('Claude', 26)\n",
      "('Come', 27)\n",
      "('Croft', 28)\n",
      "('Destroyed', 29)\n",
      "('Devonshire', 30)\n",
      "('Don', 31)\n",
      "('Dubarry', 32)\n",
      "('Emperors', 33)\n",
      "('Florence', 34)\n",
      "('For', 35)\n",
      "('Gallery', 36)\n",
      "('Gideon', 37)\n",
      "('Gisburn', 38)\n",
      "('Gisburns', 39)\n",
      "('Grafton', 40)\n",
      "('Greek', 41)\n",
      "('Grindle', 42)\n",
      "('Grindles', 43)\n",
      "('HAD', 44)\n",
      "('Had', 45)\n",
      "('Hang', 46)\n",
      "('Has', 47)\n",
      "('He', 48)\n",
      "('Her', 49)\n",
      "('Hermia', 50)\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i>= 50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf30c77",
   "metadata": {},
   "source": [
    "<!-- when we want to convert the outputs of an LLM from numbers back into\n",
    "text we also need a way to turn token IDs into text.For this we can create an inverse\n",
    "version of the vocabulary that maps token IDs back to corresponding text tokens -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3247d71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "    \n",
    "    def encode(self,text):\n",
    "        preprocessed = preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)',text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.split()]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "    def decode(self,ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        #Replace spaces before specified punctuations\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])',r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd5cb3d",
   "metadata": {},
   "source": [
    "<!-- Let's instantiate a new tokenizer object from the SimpleTokenizerV1 class and tokenize a text or a passage -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "169d0fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "text = \"\"\"\"It's the last he painted, you know,\" \n",
    "           Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "\n",
    "ids = tokenizer.encode(text)\n",
    "\n",
    "print(ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce37453f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\" It\\' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b57a7c2",
   "metadata": {},
   "source": [
    "<!-- The problem is that the word Hello was not used in the The Verdict short story\n",
    "Hence it is not contained in the vocabulary\n",
    "This highlights the need to consider large and diverse training sets to extend the vocabulary when working on LLMs\n",
    "\n",
    " -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a69faff3",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Hello'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m text = \u001b[33m\"\u001b[39m\u001b[33mHello, do you like tea?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mSimpleTokenizerV1.encode\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m      7\u001b[39m preprocessed = preprocessed = re.split(\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33m([,.:;?_!\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m()\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[33m]|--|\u001b[39m\u001b[33m\\\u001b[39m\u001b[33ms)\u001b[39m\u001b[33m'\u001b[39m,text)\n\u001b[32m      8\u001b[39m preprocessed = [item.strip() \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m preprocessed \u001b[38;5;28;01mif\u001b[39;00m item.split()]\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m ids = \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstr_to_int\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpreprocessed\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m      7\u001b[39m preprocessed = preprocessed = re.split(\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33m([,.:;?_!\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m()\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[33m]|--|\u001b[39m\u001b[33m\\\u001b[39m\u001b[33ms)\u001b[39m\u001b[33m'\u001b[39m,text)\n\u001b[32m      8\u001b[39m preprocessed = [item.strip() \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m preprocessed \u001b[38;5;28;01mif\u001b[39;00m item.split()]\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m ids = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstr_to_int\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m preprocessed]\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
      "\u001b[31mKeyError\u001b[39m: 'Hello'"
     ]
    }
   ],
   "source": [
    "text = \"Hello, do you like tea?\"\n",
    "print(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c28343",
   "metadata": {},
   "source": [
    "<!--We will modify this tokenizer to handle unknown\n",
    "words.\n",
    "In particular, we will modify the vocabulary and tokenizer we implemented in the\n",
    "previous section, SimpleTokenizerV2, to support two new tokens, <|unk|> and\n",
    "<|endoftext|> -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f708b404",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend([\"|endoftext|\",\"|unk|\"])\n",
    "\n",
    "vocab = {token:integer for integer,token in enumerate(all_tokens)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "44163123",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1132"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7eef47",
   "metadata": {},
   "source": [
    "#Printing the last two Entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bae825e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV2:\n",
    "    def __init__(self,vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = { i:s for s,i in vocab.items()}\n",
    "    def encode(self,text):\n",
    "        preprocessed = preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)',text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.split()]\n",
    "        preprocessed = [\n",
    "            item if item in self.str_to_int\n",
    "            else \"|unk|\" for item in preprocessed\n",
    "        ]\n",
    "\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "    def decode(self,ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        #Replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])',r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9bdc8879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea?<|endoftext|>In the sunlit morning terraces of the palace\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "\n",
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit morning terraces of the palace\"\n",
    "\n",
    "text = \"<|endoftext|>\".join([text1,text2])\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1af17aa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1131, 5, 355, 1126, 628, 975, 10, 1131, 988, 956, 1131, 984, 722, 988, 1131]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d5c6c2",
   "metadata": {},
   "source": [
    "#[BOS] (beginning of sequence): This token marks the start of a text. It\n",
    "signifies to the LLM where a piece of content begins.\n",
    "\n",
    "#[EOS] (end of sequence): This token is positioned at the end of a text,\n",
    "and is especially useful when concatenating multiple unrelated texts,\n",
    "similar to <|endoftext|>. For instance, when combining two different\n",
    "Wikipedia articles or books, the [EOS] token indicates where one article\n",
    "ends and the next one begins.\n",
    "\n",
    "#[PAD] (padding): When training LLMs with batch sizes larger than one,\n",
    "the batch might contain texts of varying lengths. To ensure all texts have\n",
    "the same length, the shorter texts are extended or \"padded\" using the\n",
    "[PAD] token, up to the length of the longest text in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ea8ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The tokenizer used for GPT models also doesnt use an <|unk|> token for breakin down words into subword \n",
    "# It uses Byte pair encoding not <|unk|>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fa8e2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9ecad0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8d509f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6c2410",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4160c945",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203dc799",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3c3121",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
